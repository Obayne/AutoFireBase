#Requires -Version 7.0
<#
.SYNOPSIS
    Enterprise-grade automation suite for LV CAD
.DESCRIPTION
    Comprehensive automation including security, performance, and quality checks
.PARAMETER Mode
    Automation mode: security, performance, quality, all
.PARAMETER Scan
    Enable security scanning
.PARAMETER Benchmark
    Enable performance benchmarking
#>

param(
    [ValidateSet('security', 'performance', 'quality', 'all')]
    [string]$Mode = "all",
    [switch]$Scan = $false,
    [switch]$Benchmark = $false
)

$ErrorActionPreference = "Stop"
Set-Location $PSScriptRoot\..

function Write-Header($msg) { Write-Host "`n=== $msg ===" -ForegroundColor Cyan }
function Write-Success($msg) { Write-Host "âœ“ $msg" -ForegroundColor Green }
function Write-Warning($msg) { Write-Host "âš  $msg" -ForegroundColor Yellow }
function Write-Error($msg) { Write-Host "âœ— $msg" -ForegroundColor Red }
function Write-Info($msg) { Write-Host "â„¹ $msg" -ForegroundColor Blue }

Write-Header "ğŸ¢ LV CAD Enterprise Automation Suite"

$startTime = Get-Date
Write-Info "Started at: $startTime"

# Activate virtual environment
if (Test-Path ".venv\Scripts\Activate.ps1") {
    . .venv\Scripts\Activate.ps1
}

# Install enterprise tools if needed
Write-Info "Ensuring enterprise tools are installed..."
pip install --quiet flake8 pylint bandit safety sphinx coverage pytest-benchmark selenium webdriver-manager

# Security Analysis
if ($Mode -in @('security', 'all') -or $Scan) {
    Write-Header "ğŸ”’ Security Analysis"

    Write-Info "Running Bandit security scanner..."
    try {
        $banditResult = bandit -r app cad_core frontend -f json -o security_report.json 2>$null
        if (Test-Path "security_report.json") {
            $securityIssues = Get-Content "security_report.json" | ConvertFrom-Json
            $issueCount = $securityIssues.results.Count
            if ($issueCount -gt 0) {
                Write-Warning "Found $issueCount security issues"
                $securityIssues.results | ForEach-Object {
                    Write-Host "  $($_.filename):$($_.line_number) - $($_.issue_text)"
                }
            } else {
                Write-Success "No security issues found"
            }
        }
    } catch {
        Write-Warning "Bandit not available - install with: pip install bandit"
    }

    Write-Info "Checking for vulnerable dependencies..."
    try {
        $safetyResult = safety check --json | ConvertFrom-Json
        if ($safetyResult.Count -gt 0) {
            Write-Warning "Found $($safetyResult.Count) vulnerable dependencies"
            $safetyResult | ForEach-Object {
                Write-Host "  $($_.package): $($_.vulnerable_spec) - $($_.advisory)"
            }
        } else {
            Write-Success "No vulnerable dependencies found"
        }
    } catch {
        Write-Warning "Safety not available - install with: pip install safety"
    }

    Write-Success "Security analysis completed"
}

# Performance Benchmarking
if ($Mode -in @('performance', 'all') -or $Benchmark) {
    Write-Header "âš¡ Performance Benchmarking"

    Write-Info "Running performance benchmarks..."
    try {
        # Create benchmark test if it doesn't exist
        if (-not (Test-Path "tests/test_benchmark.py")) {
            @"
import pytest
import time
from app.main import MainWindow

@pytest.mark.benchmark
def test_app_startup_performance(benchmark):
    def startup():
        # Simulate app startup
        time.sleep(0.1)  # Placeholder for actual startup time
        return True

    result = benchmark(startup)
    assert result == True

@pytest.mark.benchmark
def test_file_operations(benchmark):
    def file_ops():
        # Simulate file operations
        import tempfile
        import os
        with tempfile.NamedTemporaryFile(delete=False) as f:
            f.write(b"test data")
            temp_file = f.name

        with open(temp_file, 'r') as f:
            data = f.read()

        os.unlink(temp_file)
        return len(data)

    result = benchmark(file_ops)
    assert result == 9
"@ | Out-File "tests/test_benchmark.py" -Encoding UTF8
        }

        pytest tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark_results.json -q
        Write-Success "Performance benchmarks completed"
    } catch {
        Write-Warning "Benchmarking failed - install pytest-benchmark"
    }
}

# Code Quality Analysis
if ($Mode -in @('quality', 'all')) {
    Write-Header "ğŸ” Code Quality Analysis"

    Write-Info "Running Flake8..."
    try {
        flake8 app cad_core frontend --max-line-length=100 --extend-ignore=E203,W503
        Write-Success "Flake8 checks passed"
    } catch {
        Write-Warning "Flake8 issues found"
    }

    Write-Info "Running Pylint..."
    try {
        pylint app cad_core frontend --rcfile=.pylintrc --output-format=text | Out-Null
        Write-Success "Pylint checks completed"
    } catch {
        Write-Warning "Pylint issues found"
    }

    Write-Info "Generating coverage report..."
    try {
        coverage run --source=app,cad_core,frontend -m pytest tests/ -q
        coverage report --show-missing
        coverage html -d coverage_html
        Write-Success "Coverage report generated"
    } catch {
        Write-Warning "Coverage analysis failed"
    }
}

# Documentation Generation
if ($Mode -in @('quality', 'all')) {
    Write-Header "ğŸ“š Documentation Generation"

    Write-Info "Setting up Sphinx documentation..."
    if (-not (Test-Path "docs/conf.py")) {
        sphinx-quickstart docs --quiet --project="LV CAD" --author="AutoFire" --release="0.6.8" --language="en" --suffix=".rst" --master="index" --ext-autodoc --ext-doctest --ext-intersphinx --ext-todo --ext-coverage --ext-imgmath --ext-mathjax --ext-ifconfig --ext-viewcode --ext-githubpages --makefile --no-batchfile
    }

    Write-Info "Building Sphinx documentation..."
    try {
        sphinx-build -b html docs docs/_build/html
        Write-Success "Sphinx documentation built"
    } catch {
        Write-Warning "Sphinx documentation failed"
    }
}

# Integration Testing
if ($Mode -in @('quality', 'all')) {
    Write-Header "ğŸ”— Integration Testing"

    Write-Info "Running integration tests..."
    try {
        # Test application startup
        $appTest = python -c "import app.main; print('App imports successfully')" 2>&1
        if ($appTest -match "successfully") {
            Write-Success "Application integration test passed"
        } else {
            Write-Error "Application integration test failed"
        }

        # Test build process
        if (Test-Path "LV_CAD.spec") {
            Write-Success "Build configuration present"
        } else {
            Write-Warning "Build configuration missing"
        }

    } catch {
        Write-Warning "Integration testing failed"
    }
}

# Final Report
Write-Header "ğŸ“Š Enterprise Automation Report"

$endTime = Get-Date
$duration = $endTime - $startTime

$report = @"
ğŸ¢ Enterprise Automation Results:
   â±ï¸  Duration: $([math]::Round($duration.TotalSeconds, 1)) seconds
   ğŸ“… Started: $startTime
   ğŸ Finished: $endTime
   ğŸ¯ Mode: $Mode

ğŸ” Quality Checks:
   $(if ($Mode -in @('quality', 'all')) { "âœ… Code quality analysis completed" } else { "â­ï¸  Skipped" })

ğŸ”’ Security Analysis:
   $(if ($Mode -in @('security', 'all') -or $Scan) { "âœ… Security scanning completed" } else { "â­ï¸  Skipped" })

âš¡ Performance Testing:
   $(if ($Mode -in @('performance', 'all') -or $Benchmark) { "âœ… Performance benchmarks completed" } else { "â­ï¸  Skipped" })

ğŸ“š Documentation:
   $(if ($Mode -in @('quality', 'all')) { "âœ… Documentation generated" } else { "â­ï¸  Skipped" })

ğŸ“¦ Generated Artifacts:
   â€¢ Security Report: security_report.json
   â€¢ Benchmark Results: benchmark_results.json
   â€¢ Coverage Report: coverage_html/index.html
   â€¢ Documentation: docs/_build/html/index.html

ğŸš€ Enterprise-grade automation completed!
"@

Write-Host $report -ForegroundColor Green

Write-Host "`nğŸ† Enterprise automation suite finished!`n" -ForegroundColor Magenta
